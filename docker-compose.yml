services:
  tg-handler:
    image: veotri/tg-handler:v0.5.1
    build:
      context: .
      dockerfile: tg-handler/Dockerfile
    environment:
      API_KEYS_FILE: /run/secrets/api_keys
    volumes:
      - ./confs:/app/confs:ro
      - ./history:/app/history 
    networks:
      - ollama-network
    secrets:
      - api_keys
    depends_on:
      ollama:
        condition: service_healthy

  ollama:
    image: ollama/ollama:latest
    container_name: ollama
    volumes:
      - ollama-data:/root/.ollama
    networks:
      - ollama-network
    ports:
      - "11435:11434"
    restart: unless-stopped
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    entrypoint: ["/bin/sh", "-c", "\
      echo 'Starting server...' && \
      ollama serve & sleep 5 && \
      echo 'Pulling model...' && \
      ollama pull huihui_ai/qwen3-abliterated:14b-q8_0 && \
      echo 'Loading model...' && \
      echo 'no thinking, reply with \".\"' | ollama run huihui_ai/qwen3-abliterated:14b-q8_0 > /dev/null &&
      echo 'Model loaded into memory' &&
      wait"]
    healthcheck: # Wait for 3 hours and 5 minutes
      test: ["CMD","sh", "-c", "ollama ps | grep -q 'huihui_ai/qwen3-abliterated:14b-q8_0'"]
      interval: 10s
      timeout: 10s
      retries: 1000
      start_period: 300s

volumes:
  ollama-data:

networks:
  ollama-network:

secrets:
  api_keys:
    file: api_keys.txt
