=== File: ./validate.py ===
"""LLM Output Validation Module.

Validate module implements multi-criteria quality assurance for LLM responses
through lexical analysis and context-aware repetition detection.

Key Components:
- Length Enforcement:   Context-relative minimum response length requirements
- Content Policies:     Web reference detection, excessive questioning filters
- Repetition Checks:    Phrase-level Jaccard index repetition comparisons

Implementation Notes:
- Phrase-Based Analysis: Sentence splitting via terminal punctuation boundaries
- Threshold-Driven:      Configurable MAX_JACCARD_IDX and MIN_MSG_LEN constants
"""

import re

MAX_JACCARD_IDX = 0.1
MIN_MSG_LEN = 2 / 3


def _split_to_phrases(text: str) -> list[str]:
    """Split phrases based on delimeters."""
    delimiters = (".", "!", "?")
    pattern = "|".join(map(re.escape, delimiters))
    return re.split(pattern, text)


def _get_jaccard_idx(str1: str, str2: str) -> float:
    """Calculate Jaccard index to measure how one string repeats the other.

    Args:
        str1: str
        str2: str

    Returns: float
    """
    jaccard_idx = 0

    set1 = set(str1.split())
    set2 = set(str2.split())

    intersection_length = len(set1.intersection(set2))
    union_length = len(set1.union(set2))

    if union_length != 0:
        jaccard_idx = intersection_length / union_length

    return jaccard_idx


def _is_itself_repetitive(response: str) -> bool:
    """Check if response is repetitive to itself via Jaccard index.

    Args:
        response: str

    Returns: bool
    """
    jaccard_idx = 0
    phrases = _split_to_phrases(response)

    for i in range(len(phrases)):
        for j in range(i + 1, len(phrases)):
            jaccard_idx = _get_jaccard_idx(phrases[i], phrases[j])
            if jaccard_idx > MAX_JACCARD_IDX:
                return True

    return False


def _is_prompt_repetitive(response: str, prompt: str) -> bool:
    """Check if response is repetitive to prompt via Jaccard index.

    Args:
        response: str
        prompt: str

    Returns: bool
    """
    jaccard_idx = 0

    response_phrases = _split_to_phrases(response)
    prompt_phrases = _split_to_phrases(prompt)

    for prompt_phrase in prompt_phrases:
        for response_phrase in response_phrases:
            jaccard_idx = _get_jaccard_idx(prompt_phrase, response_phrase)
            if jaccard_idx > MAX_JACCARD_IDX:
                return True

    return False


def _is_dialog_repetitive(response: str, dialog: list[str]) -> bool:
    """Check if response is repetitive to dialog via Jaccard index.

    Args:
        response: str
        dialog: list[str]

    Returns: bool
    """
    jaccard_idx = 0

    response_phrases = _split_to_phrases(response)

    for message in dialog:
        message_phrases = _split_to_phrases(message)

        for message_phrase in message_phrases:
            for response_phrase in response_phrases:
                jaccard_idx = _get_jaccard_idx(message_phrase, response_phrase)
                if jaccard_idx > MAX_JACCARD_IDX:
                    return True

    return False


def _is_repetitive(response: str, prompt: str, dialog: list[str]) -> bool:
    """Check if response is repetitive to itself, prompt or dialog.

    Args:
        response: str
        propmt: str
        dialog: list[str]

    Returns: bool
    """
    is_repetitive = False

    is_repetitive = _is_itself_repetitive(response)
    if is_repetitive:
        return is_repetitive

    is_repetitive = _is_prompt_repetitive(response, prompt)
    if is_repetitive:
        return is_repetitive

    is_repetitive = _is_dialog_repetitive(response, dialog)
    if is_repetitive:
        return is_repetitive

    return is_repetitive


def validate(resp: str, prompt: str, dialog: list[str]) -> tuple[bool, str]:
    """Check if response is web-related, questioning, short or repetitive.

    Args:
        response: str
        propmt: str
        dialog: list[str]

    Returns: tuple[bool, str]

    Note: prompt should be from user, as checking against system prompt
    can invalidate basic self-presentation.
    """
    err = ""
    last_msg = dialog[-1]

    web = "[RETEJO]" in resp
    short = len(resp) < (len(last_msg) * MIN_MSG_LEN)
    maybe_dialog = ":" in resp
    # repetitive = _is_repetitive(resp, prompt, dialog)

    if web:
        err += " <Web>"
    if short:
        err += " <Short>"
    if maybe_dialog:
        err += " <Maybe Dialog>"
    # if repetitive:
    #    err += " <Repetitive>"

    bad = web or short or maybe_dialog  # or repetitive
    ok = not bad

    return ok, err

=== File: ./selector.py ===
"""LLM Response Selection Module.

Selector subclass implements batch rating and quality-based response selection
through multi-attempt validation and statistical rate averaging.

Key Components:
- Rating Pipeline:   Regex-based score extraction with validity checks
- Batch Rating:      Parallel rating attempts with tqdm progress tracking
- Quality Selection: Descending-order response ranking with colored output

Note:
- Rate Limits: Strict [0-10] rating bounds with regex filtering
- Mode: 'rate' as the LLM part only rates, while selection is done by Selector
"""

import re

from colorama import Fore, init
from tqdm.auto import tqdm

from generator import Generator

init(autoreset=True)


class Selector(Generator):
    """Selector uses Generator initialize method.

    self.select() to rate response candidates and return the best one.
    """

    OVERFLOW_MSG = "Fatal: 10 times the batch size. Skipped generations."
    FATAL_MSG = "No rates generated with prompt: %s."
    MIN_RATE = 0
    MAX_RATE = 10

    def _to_rate(self, response: str) -> tuple[str, bool]:
        """Convert response to rate.

        Args:
            self
            response: str

        Returns: tuple[str, bool]
        """
        ok = True

        # Export rate from "Rate: 5.5/10"
        response = re.sub(r"^[Rr]ate:\s?", "", response)
        response = re.split(r"(?:\s|\.|/)", response)[0]

        digits = ''.join(char for char in response if char.isdigit())
        if not digits:
            ok = False
            return 0, ok

        rate = int(digits)
        if rate < self.MIN_RATE or rate > self.MAX_RATE:
            ok = False

        return rate, ok

    def _rate_average(self, pbar, response: str, idx: int, size: int) -> float:
        """Calculate average rate from up to batch size rates.

        Args:
            self
            pbar
            response: str
            idx: int (response index)
            size: int (batch size)

        Returns: float

        Raises
        ------
            ValueError: problem
        """
        rates = []

        settings = self._settings

        user_prompt = settings.rate_prompt.format(
            self._reply_chain_str, response)
        rate_batch_size = settings.rate_batch_size

        max_attempts = rate_batch_size * 10
        for attempt in range(max_attempts):
            # Update progress bar description with current status
            pbar.set_description(f"Batch: {idx:02}/{size:02}")
            pbar.set_postfix_str(f"Try: {attempt + 1:02}/{max_attempts:02}")

            rate_raw = self.generate(user_prompt)
            rate, ok = self._to_rate(rate_raw)

            # Update main progress bar if okay
            if ok:
                rates.append(rate)
                pbar.update(1)
                if len(rates) >= rate_batch_size:
                    break
        if len(rates) < rate_batch_size:
            pbar.write(self.OVERFLOW_MSG)

        if len(rates) == 0:
            raise ValueError(self.FATAL_MSG % user_prompt)

        # Clear attempt information when moving to next response
        pbar.set_postfix_str()

        average_rate = sum(rates) / len(rates) if rates else 0.0

        return average_rate

    def _rate_average_all(self) -> list[str]:
        """Calculate average rates for all responses with single progress bar.

        Args:
            self

        Returns: list[str]
        """
        rates = []
        responses = self.responses or []

        rate_batch_size = self._settings.rate_batch_size

        total_steps = len(responses) * rate_batch_size
        with tqdm(total=total_steps, desc="Total Progress") as pbar:
            for idx, response in enumerate(responses, 1):
                rate = self._rate_average(pbar, response, idx, len(responses))
                rates.append(rate)

        return rates

    def select(self) -> str:
        """Select the best response and print response in descending order.

        Args:
            self

        Returns: str
        """
        best_response = ""

        rates = self._rate_average_all()
        responses = self.responses or []

        verbose = self._llm.verbose

        zipped = zip(rates, responses)
        sorted_zipped = sorted(zipped, key=lambda x: x[0], reverse=True)
        for i, (rate, response) in enumerate(sorted_zipped):
            if i == 0:
                best_response = response
            if verbose:
                colorer = Fore.LIGHTGREEN_EX if i == 0 else ""
                print(f"{colorer}Rate: {rate:.1f}/10\n{response}\n")
            else:
                break

        return best_response

=== File: ./Dockerfile ===
FROM nvidia/cuda:13.0.2-cudnn-runtime-ubuntu24.04

WORKDIR /app

RUN apt update && \
    apt install --no-install-recommends -y build-essential ca-certificates software-properties-common && \
    apt install --no-install-recommends -y python3.12 python3.12-dev python3.12-venv python3-pip python3-setuptools

ENV VIRTUAL_ENV=/opt/venv
RUN python3 -m venv $VIRTUAL_ENV
ENV PATH="$VIRTUAL_ENV/bin:$PATH"

COPY ./llm-server/requirements.txt .
RUN python3.12 -m pip install --upgrade pip && \
    python3.12 -m pip install --no-cache-dir -r requirements.txt

COPY ./llm-server .

EXPOSE 8000

CMD ["uvicorn", "app:app" , "--host", "0.0.0.0", "--port", "8000", "--reload"]

=== File: ./clean.py ===
"""LLM Output Sanitization Module.

Clean module implements multi-stage text normalization for raw LLM output,
enforcing formatting rules and grammatical correctness via regex-processing.

Key Components:
- EOS Handling:    Stripping of verbalized end-of-sequence markers (EOS)
- Role Removal:    Speaker identifier and preceding context elimination
- Structurization: Enforcing complete sentences and proper capitalization
- Spacing Fix:     Punctuation spacing and line breaks standartization

Implementation Notes:
- Order Sensitive:   Dependent on the order of applied operations on raw output
- Idempotent Design: Safe for repeated application on same text
"""

import re


def _strip_verbalized_eos(text: str) -> str:
    text = text.split("EOS")[0]
    return text


def _strip_extra_names(text: str) -> str:
    """Strip regex-matches of role identifyers."""
    text = re.sub(r"^.{0,24}:\s*", "", text)

    return text


def _strip_incomplete_sentences(text: str) -> str:
    """Strip everything after last valid sentence punctuation sign (.!?)."""
    if text:
        text = text[0].upper() + text[1:]

    last_index = max((text.rfind(char) for char in ".!?"), default=-1)
    if last_index != -1:
        text = text[: last_index + 1]

    return text


def _fix_punctuation(text: str) -> str:
    """Adjust all punctuation signs according to the norms."""
    # Dots
    text = re.sub(r"\s*\.", ".", text)
    # Commas
    text = re.sub(r"\s*,\s*", ", ", text)

    # Spaces
    pattern = r"\s*([.!?]\s*)(\S)"

    def fix(m: re.Match[str]) -> str:
        punct = m.group(1)
        letter = m.group(2)
        return f"{punct.strip()} {letter.upper()}"

    text = re.sub(pattern, fix, text)

    return text


def _sub_custom_n(text: str) -> str:
    r"""Substitute escaped \\n with new line character."""
    text = re.sub(r"\\n", "\n", text)
    return text


def clean(text: str) -> str:
    """Clean LLM generated text."""
    text = _strip_verbalized_eos(text)
    text = _strip_extra_names(text)
    text = _strip_incomplete_sentences(text)

    text = _fix_punctuation(text)
    text = _sub_custom_n(text)

    return text

=== File: ./app.py ===
#!/usr/bin/env python

"""LLM Inference Server App.

FastAPI-based web service for tunable LLM inference.
Two-stage processing pipeline with configurable parameters.

Key Components:
1. API Endpoint:
   - POST /v1/generate: RequestBody(dialog, settings) -> ResponseBody(response)

2. Processing:
   - LLM Inference: Load LLM from './model' according to its config
   - Response Generation: Use Responder class to generate response candidates
   - Response Selection: Use Selector class to select response from candidates
   - Timing Metrics: Track processing time

3. Error Handling:
   - Return ResponseBody(ERR_MSG) for:
     * No response/rate candidates generated

Usage Example:
    $ uvicorn app:app --host 0.0.0.0 --port 8000 --reload
    POST /v1/generate with JSON body containing dialog and settings
"""

import timeit
from collections.abc import Callable
from functools import wraps

import torch
from fastapi import FastAPI
from transformers import (
    AutoTokenizer,
    BitsAndBytesConfig,
    MllamaForConditionalGeneration,
)

from models import LLM, RequestBody, ResponseBody
from responder import Responder
from selector import Selector

MODEL = "./model"
ERR_MSG = "Server error."

tokenizer = AutoTokenizer.from_pretrained(MODEL)
model = MllamaForConditionalGeneration.from_pretrained(
    MODEL,
    quantization_config=BitsAndBytesConfig(load_in_8bit=True),
    torch_dtype=torch.float16,
    device_map="cuda",
)


llm = LLM(
    model=model,
    tokenizer=tokenizer,
    mode=None,
    verbose=True
)

app = FastAPI()


def timing(func: Callable) -> Callable:
    """Measure function runtime."""

    @wraps(func)
    def inner(*args: RequestBody) -> dict[str, str]:
        ts = timeit.default_timer()

        result = func(*args)

        te = timeit.default_timer()
        time = int(te - ts)

        minutes, seconds = time // 60, time % 60
        print(f"Time generating: {minutes}m {seconds}s")

        return result

    return inner


@timing
def generate(request: RequestBody) -> ResponseBody:
    """Generate response with specified LLM parameters."""
    error = ResponseBody(response=ERR_MSG)

    # base data objects
    settings = request.settings
    chat_context = request.chat_context
    reply_chain = request.reply_chain

    # Generate responses
    responses = []
    llm.mode = "response"
    responder = Responder(llm, settings, chat_context,
                          reply_chain, responses=None)
    try:
        responses = responder.respond()
    except ValueError:
        return error

    # Select best response via rating
    response_str = ""
    llm.mode = "rate"
    selector = Selector(llm, settings, chat_context, reply_chain, responses)
    try:
        response_str = selector.select()
    except ValueError:
        return error

    return ResponseBody(response=response_str)


@app.post("/v1/generate")
async def chat(request: RequestBody) -> ResponseBody:
    """Return response from LLM Server."""
    return generate(request)

=== File: ./requirements.txt ===
bitsandbytes==0.45.3
colorama==0.4.6
fastapi==0.115.11
numpy==2.2.3
peft==0.14.0
ruff==0.9.9
tqdm==4.67.1
transformers==4.49.0
uvicorn==0.34.0

=== File: ./pyproject.toml ===
[project]
requires-python = ">=3.7"
license = { file = "LICENSE" }

[tool.ruff]
line-length = 79

[tool.ruff.lint]
select = [
	'F',       # pyflakes
	'E', 'W',  # pycodestyle
	'A', 'S',  # flake8(-builtins/-bandit)
	'PL',      # pylint
	'I',       # isort
	'N',       # pep8 naming
	'D',       # pydocstyle
	'C4',      # comprehensions
	'SIM',     # simplification
	'TCH',     # type checking
	'FAST',    # fastapi
	'NPY',     # numpy
]
ignore = [
	'S105'     # hardcoded password false positive
]

[tool.ruff.lint.pydocstyle]
convention = "numpy"

[tool.ruff.format]
docstring-code-format = true
docstring-code-line-length = 70

=== File: ./models.py ===
"""Models used for LLM server."""

from dataclasses import dataclass
from typing import Literal

from pydantic import BaseModel
from transformers import PreTrainedModel, PreTrainedTokenizer


@dataclass
class LLM:
    """Config for LLM."""

    model: PreTrainedModel
    tokenizer: PreTrainedTokenizer
    mode: Literal["response", "rate"] | None
    verbose: bool


@dataclass
class Settings:
    """Settings from bot/cmd config."""

    system_prompt: str
    chain_prompts: list[str]
    rate_prompt: str
    temperature: float
    repetition_penalty: float
    top_p: float
    top_k: int
    response_tokens: int
    response_token_shift: int
    response_batch_size: int
    rate_tokens: int
    rate_batch_size: int


class RequestBody(BaseModel):
    """Request passed to API endpoint."""

    chat_context: list[str]
    reply_chain: list[str]
    settings: Settings


class ResponseBody(BaseModel):
    """Response from API endpoint."""

    response: str

=== File: ./responder.py ===
"""Chain of Thought Response Generation Module.

Responder class manages parallelized Chain of Thought reasoning for response
generation with cleaning, validation, retries and batch processing.

Key Components:
- Responder:      Orchestration of CoT workflow with colored console feedback
- Thought Chains: Parallel reasoning paths managed via NumPy arrays
- Clean/Validate: Imported validation and cleaning pipelines for LLM output
"""

from contextlib import contextmanager

import numpy as np
from colorama import Fore, init
from numpy.typing import NDArray

from clean import clean
from generator import Generator
from validate import validate

init(autoreset=True)


class Responder(Generator):
    """Responder initialized with Generator initialize method.

    self.respond() to respond based on passed dialog.
    """

    SUCCESS_MSG = Fore.GREEN + "[Success]"
    FAILURE_MSG = Fore.RED + "[Failure]"
    OVERFLOW_MSG = Fore.RED + "[Fatal] Max attempts exceeded. Generation skip."
    FATAL_MSG = "No thoughts generated with prompt: %s."

    def _think(self, chain_prompt: str, thought_chain: NDArray) -> list[str]:
        """Generate thought based on chain prompt with thought chain."""
        thoughts = []

        settings = self._settings
        batch_size = settings.response_batch_size
        max_attempts = batch_size * 3
        verbose = self._llm.verbose

        chat_context = self._chat_context
        chat_context_str = self._chat_context_str

        reply_chain = self._reply_chain
        reply_chain_str = self._reply_chain_str

        user_prompt = chain_prompt.format(
            chat_context_str, reply_chain_str, *thought_chain)
        if verbose:
            print(user_prompt)

        for attempt in range(max_attempts):
            current_try = attempt + 1
            print(f"Try {current_try:02}:", end=" ")

            thought_raw = self.generate(user_prompt)
            thought = clean(thought_raw)

            validation_context = chat_context + reply_chain
            ok, err = validate(thought, chain_prompt, validation_context)
            if ok:
                print(self.SUCCESS_MSG)
                thoughts.append(thought)
                if verbose:
                    print(thought_raw)
                if len(thoughts) >= batch_size:
                    break
            else:
                print(self.FAILURE_MSG, end=" ")
                print(err)

        if len(thoughts) < batch_size:
            print(self.OVERFLOW_MSG)

        if len(thoughts) == 0:
            raise ValueError(self.FATAL_MSG % chain_prompt)

        return thoughts

    @contextmanager
    def _temp_batch_size(self, new_size: int) -> None:
        """Temporarily modify the batch size."""
        original = self._settings.response_batch_size
        self._settings.response_batch_size = new_size
        try:
            yield
        finally:
            self._settings.response_batch_size = original

    def respond(self) -> list[str]:
        """Implement Chain of Thought algorithm."""
        settings = self._settings
        chain_prompts = settings.chain_prompts
        batch_size = settings.response_batch_size

        # Generate initial thoughts for all parallel chains
        print("Step 1: CoT start")
        chains = [[] for _ in range(batch_size)]  # Each chain starts empty
        initial_thoughts = self._think(chain_prompts[0], np.array([]))

        # Distribute initial thoughts to chains
        for chain_idx, thought in enumerate(initial_thoughts):
            if chain_idx < batch_size:
                chains[chain_idx].append(thought)

        if len(chain_prompts) == 1:
            return [chain[-1] for chain in chains if chain]

        # Continue each chain through remaining steps
        print("Step 2: CoT continue")
        with self._temp_batch_size(1):
            for step_idx, chain_prompt in enumerate(chain_prompts[1:], 1):
                for chain_idx, chain in enumerate(chains):
                    if len(chain) >= step_idx:  # Chain exists and is ready
                        new_thoughts = self._think(
                            chain_prompt, np.array(chain))
                        if new_thoughts:
                            chains[chain_idx].append(new_thoughts[0])

        # Return the final thought from each chain
        return [chain[-1] for chain in chains if chain]

=== File: ./stopper.py ===
"""LLM Generation Termination Module.

Termination module provides dialog-aware early stopping mechanisms fortext
generation through dynamic stop sequence detection and post-processing cleanup.

Key Components:
- Sequence Detection:  Real-time token sequence matching during generation
- Dialog Processing:   Speaker identifyer extraction from conversation history
- Output Sanitization: Post-generation removal of matched stop sequences
"""

import torch
from transformers import StoppingCriteria, StoppingCriteriaList


class StopOnTokens(StoppingCriteria):
    """Dynamic stopping criteria based on token sequence detection.

    Monitors generation output for predefined token sequences, flagging
    sequences for termination when any configured stop pattern is matched.

    Args:
        stop_token_ids: List of token sequences to detect (list[list[int]])

    Methods_
        __call__: Batch-enabled sequence matching check
    """

    def __init__(self, stop_token_ids: list):
        super().__init__()
        self.stop_token_ids = stop_token_ids

    def __call__(self, input_ids: torch.LongTensor,
                 _scores: torch.FloatTensor, **kwargs) -> torch.BoolTensor:
        """Evaluate current generation state for stop sequence matches.

        Args:
            input_ids: torch.LongTensor
            _scores: torch.FloatTensor

        Returns_
            torch.BoolTensor
        """
        # Initialize all sequences as not stopping
        stop_flags = torch.zeros((input_ids.shape[0],),
                                 dtype=torch.bool, device=input_ids.device)

        for stop_seq in self.stop_token_ids:
            if len(stop_seq) == 0:
                continue
            # Convert stop sequence to tensor on correct device
            stop_tokens = torch.tensor(stop_seq, device=input_ids.device)
            seq_len = stop_tokens.shape[0]

            # Skip if input is shorter than current stop sequence
            if input_ids.shape[-1] < seq_len:
                continue

            # Get last 'seq_len' tokens from input
            window = input_ids[:, -seq_len:]
            # Check for match with current stop sequence
            matches = (window == stop_tokens).all(dim=1)
            # Update stop flags
            stop_flags |= matches

        return stop_flags


def get_stop_vars(tokenizer, dialog):
    """Generate dialog-derived stopping criteria and token sequences.

    Args:
        tokenizer: HF Tokenizer for text->token conversion
        dialog: Conversation history for stop pattern extraction

    Returns_
        tuple: (configured stopping criteria, detected stop token sequences)

    Processes dialog lines to extract speaker prefixes as stop patterns.
    Example: "User: Hello" -> stop sequence ["U", "ser", ":"]
    """
    stop_strings = []
    for line in dialog:
        colon_pos = line.find(":")
        if colon_pos > 0:  # Only consider lines with non-empty speaker names
            stop_str = line[:colon_pos+1]
            if len(stop_str) > 1:  # Exclude patterns like ":"
                stop_strings.append(stop_str)

    stop_token_ids = [
        tokenizer.encode(stop_string, add_special_tokens=False)
        for stop_string in stop_strings
    ]

    stopping_criteria = StoppingCriteriaList([StopOnTokens(stop_token_ids)])

    return stopping_criteria, stop_token_ids


def trim_stop_sequences(output_ids, stop_token_ids):
    """Remove detected stop sequences from generated outputs.

    Args:
        output_ids: Raw generated token sequences
        stop_token_ids: Stop patterns detected during generation

    Returns_
        list: Cleaned token sequences with stop patterns removed

    Longest-match-first removal prevents partial sequence retention.
    Processes sequences in reverse length order for optimal pattern erasure.
    """
    processed = []
    for output_id in output_ids:
        seq = output_id.tolist()
        removed = 0

        # Check all stop sequences from longest to shortest
        for stop_seq in sorted(stop_token_ids, key=len, reverse=True):
            stop_len = len(stop_seq)
            if stop_len == 0:
                continue

            # Check if sequence ends with this stop pattern
            if len(seq) >= stop_len and seq[-stop_len-removed:] == stop_seq:
                removed += stop_len

        processed.append(seq[:-removed] if removed > 0 else seq)

    return processed

=== File: ./list.txt ===

=== File: ./generator.py ===
"""LLM Text Generation Base Module.

Generator superclass orchestrates dialog processing, token management, and
LLM inference for specialized Responder/Selector subclasses.

Key Components:
- Dialog Processing:   Normalization and string convertion
- Token Management:    Mode-based configuration (respond/rate)
- Generation Pipeline: Prompt templating and stop sequence utilization
"""

import re
from typing import Optional

import torch

from models import LLM, Settings
from stopper import get_stop_vars, trim_stop_sequences


class Generator:
    """Superclass for Responder and Selector classes.

    self.generate(user_prompt: str) to generate LLM response.
    """

    SYSTEM_TEMPLATE = "<|start_header_id|>system<|end_header_id|>%s<|eot_id|>"
    USER_TEMPLATE = "<|start_header_id|>user<|end_header_id|>%s<|eot_id|>"
    ASSISTANT_TEMPLATE = "<|start_header_id|>assistant<|end_header_id|>"

    RESP_TOKEN_ERR_MSG = "No token number specified for response."
    MODE_ERR_MSG = "Mode is %s. Set to 'respond' or 'rate'!"
    RATE_TOKEN_ERR_MSG = "No token number for rate."

    def __init__(
        self,
        llm: LLM,
        settings: Settings,
        chat_context: list[str],
        reply_chain: list[str],
        responses: Optional[list[str]],
    ) -> None:
        self._llm = llm
        self._settings = settings
        self._chat_context = self.normalize_text(chat_context)
        self._chat_context_str = "\n".join(chat_context)
        self._reply_chain = self.normalize_text(reply_chain)
        self._reply_chain_str = "\n".join(reply_chain)

        self._set_stopping_criteria()
        self._set_token_num()

        self.responses = responses

    @staticmethod
    def normalize_text(text: list[str]) -> list[str]:
        for i, msg in enumerate(text):
            if "\n" in msg:
                text[i] = re.sub("\n+", r"\\n", msg)
        return text

    def _set_stopping_criteria(self) -> None:
        tokenizer = self._llm.tokenizer
        reply_chain = self._reply_chain

        stopping_criteria, stop_token_ids = get_stop_vars(tokenizer,
                                                          reply_chain)
        self.stopping_criteria = stopping_criteria
        self.stop_token_ids = stop_token_ids

    def _set_token_num(self) -> None:
        """Set self.max_new_tokens token number based on mode and values.

        select mode: rate_tokens* ?
        respond mode: response_tokens*/inputs.size(1) + response_token_shift ?
        * Must be non-zero; ? Raise error on failure

        Args:
            self

        Returns: None

        Raises
        ------
            ValueError: problem

        """
        tokenizer = self._llm.tokenizer
        mode = self._llm.mode
        settings = self._settings
        reply_chain = self._reply_chain

        match mode:
            case "rate":
                rate_tokens = settings.rate_tokens
                # Set static (specified)
                if rate_tokens != 0:
                    self.max_new_tokens = settings.rate_tokens
                else:
                    err = ValueError(self.RATE_TOKEN_ERR_MSG)
                    raise err
            case "response":
                response_tokens = settings.response_tokens
                response_token_shift = settings.response_token_shift
                # Set static
                if settings.response_tokens != 0:
                    self.max_new_tokens = response_tokens
                # Set with shift
                elif settings.response_token_shift != 0:
                    inputs = tokenizer.encode(
                        reply_chain[-1], return_tensors="pt")
                    self.max_new_tokens = inputs.size(1) + response_token_shift
                else:
                    err = ValueError(self.RESP_TOKEN_ERR_MSG)
                    raise err
            case _:
                err = ValueError(self.MODE_ERR_MSG % mode)
                raise err

    def _new_prompt(self, user_prompt: str) -> str:
        settings = self._settings

        system_prompt = settings.system_prompt

        prompt = self.SYSTEM_TEMPLATE % system_prompt
        prompt += self.USER_TEMPLATE % user_prompt
        prompt += self.ASSISTANT_TEMPLATE

        return prompt

    def generate(self, user_prompt: str) -> str:
        model = self._llm.model
        tokenizer = self._llm.tokenizer
        settings = self._settings

        max_new_tokens = self.max_new_tokens
        stop_token_ids = self.stop_token_ids
        stopping_criteria = self.stopping_criteria

        prompt = self._new_prompt(user_prompt)
        inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
        with torch.no_grad():
            output_ids = model.generate(
                **inputs,
                do_sample=True,
                bos_token_id=tokenizer.bos_token_id,
                eos_token_id=tokenizer.eos_token_id,
                pad_token_id=tokenizer.pad_token_id,
                temperature=settings.temperature,
                repetition_penalty=settings.repetition_penalty,
                top_p=settings.top_p,
                top_k=settings.top_k,
                max_new_tokens=max_new_tokens,
                stopping_criteria=stopping_criteria,
            )
            processed_ids = trim_stop_sequences(output_ids, stop_token_ids)
            response_raw = tokenizer.batch_decode(
                processed_ids, skip_special_tokens=True
            )[0]
            response_raw = response_raw.split("assistant")[-1].strip()

        return response_raw

